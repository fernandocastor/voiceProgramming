# Vocal Programming for People with Upper-Body Motor Impairments, by Lucas Rosenblatt and colleagues.

The paper starts out by mentioning Benjamin Meyer's system for JavaScript voice-based programming and Travis Rudd's Python-based system (which also seems to work for LISP). There's also Silvius, by David Williams-King, for Python programming.

The emphasis of this paper seems to be very much on evaluating voice as an input modality for programming tasks by people who have limited dexterity. This is the main way in which this work differentiates itself from other papers. This is the second paper I've read which takes a "Wizard of Oz" study as its starting point. The other one was "Off to see the Wizard: using a "Wizard of Oz" study to learn how to design a spoken language interface for programming", published in 2002. In Human Computer Interaction, a **Wizard of Oz** experiment is a kind of research experiment where the subject of the study believes that she is interacting with the computing system when in reality she is interacting with a human being who is in fact operating the computing system.

In the WOz study conducted in the current paper, ten developers who do not have motor impairments gave voice commands to a researcher who simulated a voice recognition system. This researcher used a text editor to actually type the code. The results of this study were the basis for the construction of VocalIDE, their proposal for a voice-based IDE. According to the paper, *"VocalIDE supports both writing and editing, although it focuses specially on code editing."*

The paper emphasizes multiple disadvantages in the production systems of Rudd and Meyer. for example, The user must learn a host of new and non-intuitive vocal commands to be able to effectively use these systems. These commands impose a (non-quantified) overhead on the task of programming by voice, e.g., because the user has to say the letters one by one. Furthermore, the systems require a very high level of precision and demand considerable a priori keyboard set-top. Finally the systems have not been empirically validated. In addition, while discussing related work, it makes it very clear that the current state-of-the-art is lacking.

This paper seems to place a higher emphasis on navigation coma besides simply inputting text. The six coding problems that the participants of the formative study had to solve is evidence of that. In that study, the participants were presented with two code snippets and were asked to perform edits on the code snippet on the left so as to make it similar to the code snippet on the right. This is probably what the authors mean when they say that they are not placing emphasis on writing code; they mean not writing code from scratch. They justify this choice by arguing that editing clothes is a very important task in software development and that just editing the code reduces the entry barrier to this kind of experiment because the subjects might have problems that stem from actually writing code, instead of just using this alternative form of input.

Voice-based programming should support replace-and-find naturally. It is probably common for the user to state stuff such as "change [the name of] variable i to j". Integration with refactoring would be even better.

The proposed ID E uses web based speech recognition, more specifically, it leverages something called WebKitSpeechRecognition, which is natively available in WebKit-based browsers such as Google Chrome. It also uses a rule-based send text parser. The system uses predefined commands and their respective parameters to control how the system processes input. One of these commands is for text selection. That's interesting because the way the user specifies what is the part of the code to be selected is probably different from the way the user provides code input. 

*"What it does not do well is interpret commands such as 'make a new variable foo with value 3'".*

Another interesting feature is called Color Context Editing. Basically, the system highlights with different colors elements that appear either right above, right below, or to the sides of the current cursor position. These colors aim to make text selection and navigation quicker and more intuitive to the user. In the current version of the system, the authors limit the number of different colors to 9. **Navigation** really is the strongest part of VocalIDE. The list of navigation commands in Section 4 sounds really useful.

The evaluation of the proposed approach involved 8 people with motor impairments of varied degrees. None of them were coders, which means that the evaluation occurred exclusively in terms of the navigational features of VocalIDE. The participants of the study have been diagnosed with cerebral palsy, spinal cord damage, and stroke. All of these can severely limit a person's dexterity. The study evaluated the participants' ability to code according to the 9 categories established by LaToza et al.[1], which include writing, editing, designing, and unit testing, among others. 

[1] Thomas D. LaToza, Gina Venolia, and Robert DeLine. 2006. Maintaining mental models: a study of developer work habits. In Proceedings of the 28th international conference on Software engineering (ICSE '06). ACM, New York, NY, USA, 492-501. DOI: https://doi.org/10.1145/1134285.1134355

This paper has some very nice quotes that can be used to motivate the importance of programming by voice to individuals with severe motor limitations:

> The participants’ comments reflected our motivation in facilitating people with disabilities to enter the software industry. P5 noted that “[He had] always wanted to program a video game where someone’s in a wheelchair and in high school – sort of like buffy the vampire slayer meets terminator…"* and *"people in wheelchairs are not well integrated into entertainment... I’d use any software that would help me build the video game."* They also said *"I can’t wait to get your system when its on the market, it would be so helpful for writing stories. Now that I know I can voice type I will keep doing it."* P6 noted that *"I’ve always wanted to make a video game with code...now I can use my voice."