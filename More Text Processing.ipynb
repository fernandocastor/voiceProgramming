{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sky is blue and beautiful.</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this blue and beautiful sky!</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A king's breakfast has sausages, ham, bacon, eggs, toast and beans</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love green eggs, ham, sausages and bacon!</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The sky is very blue and the sky is very beautiful today</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The dog is lazy but the brown fox is quick!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             Document Category\n",
       "0                                      The sky is blue and beautiful.  weather\n",
       "1                                   Love this blue and beautiful sky!  weather\n",
       "2                        The quick brown fox jumps over the lazy dog.  animals\n",
       "3  A king's breakfast has sausages, ham, bacon, eggs, toast and beans     food\n",
       "4                         I love green eggs, ham, sausages and bacon!     food\n",
       "5                    The brown fox is quick and the blue dog is lazy!  animals\n",
       "6            The sky is very blue and the sky is very beautiful today  weather\n",
       "7                         The dog is lazy but the brown fox is quick!  animals"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "%matplotlib inline\n",
    "\n",
    "corpus = ['The sky is blue and beautiful.',\n",
    "          'Love this blue and beautiful sky!',\n",
    "          'The quick brown fox jumps over the lazy dog.',\n",
    "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
    "          'I love green eggs, ham, sausages and bacon!',\n",
    "          'The brown fox is quick and the blue dog is lazy!',\n",
    "          'The sky is very blue and the sky is very beautiful today',\n",
    "          'The dog is lazy but the brown fox is quick!'    \n",
    "]\n",
    "\n",
    "labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather', 'animals']\n",
    "\n",
    "corpus = np.array(corpus)\n",
    "corpus_df = pd.DataFrame({'Document': corpus, \n",
    "                          'Category': labels})\n",
    "corpus_df = corpus_df[['Document', 'Category']] # What's this line for?\n",
    "print(corpus_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sky blue beautiful', 'love blue beautiful sky',\n",
       "       'quick brown fox jumps lazy dog',\n",
       "       'kings breakfast sausages ham bacon eggs toast beans',\n",
       "       'love green eggs ham sausages bacon',\n",
       "       'brown fox quick blue dog lazy', 'sky blue sky beautiful today',\n",
       "       'dog lazy brown fox quick'], dtype='<U51')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc): \n",
    "    # replace occurrences of the RE pattern by '' in doc. \n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    filtered_tokens = [token for token in tokens if not(token in stop_words)]\n",
    "    lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "    filtered_lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    filtered_lemma_stem_tokens = [ps.stem(token) for token in filtered_lemmatized_tokens]\n",
    "    \n",
    "    doc = ' '.join(filtered_lemma_stem_tokens)\n",
    " #   doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "norm_corpus = normalize_corpus(corpus)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', ':', '6', 'And', 'God', 'said', ',', 'Let', 'there', 'be', 'a', 'firmament', 'in', 'the', 'midst', 'of', 'the', 'waters', ',', 'and', 'let', 'it', 'divide', 'the', 'waters', 'from', 'the', 'waters', '.']\n",
      "['1', ':', '7', 'And', 'God', 'made', 'the', 'firmament', ',', 'and', 'divided', 'the', 'waters', 'which', 'were', 'under', 'the', 'firmament', 'from', 'the', 'waters', 'which', 'were', 'above', 'the', 'firmament', ':', 'and', 'it', 'was', 'so', '.']\n",
      "['1', ':', '8', 'And', 'God', 'called', 'the', 'firmament', 'Heaven', '.']\n",
      "['And', 'the', 'evening', 'and', 'the', 'morning', 'were', 'the', 'second', 'day', '.']\n",
      "god said let firmament midst waters let divide waters waters\n",
      "god made firmament divided waters firmament waters firmament\n",
      "god called firmament heaven\n",
      "evening morning second day\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from string import punctuation\n",
    "\n",
    "bible = gutenberg.sents('bible-kjv.txt')\n",
    "\n",
    "# I think the following two lines are useless. normalize_corpus() already does that.\n",
    "#remove_terms = punctuation + '0123456789'\n",
    "#norm_bible = [[word.lower() for word in sent if not(word in remove_terms)] for sent in bible]\n",
    "# We need to make bible into a list of strings corresponding to sentences in order to \n",
    "# use normalize_corpus()\n",
    "norm_bible = [' '.join(tok_sent) for tok_sent in bible]\n",
    "norm_bible = filter(None, normalize_corpus(norm_bible))\n",
    "norm_bible = [tok_sent for tok_sent in norm_bible if len(tok_sent.split()) > 2]\n",
    "print(bible[10])\n",
    "print(bible[11])\n",
    "print(bible[12])\n",
    "print(bible[13])\n",
    "print(norm_bible[10])\n",
    "print(norm_bible[11])\n",
    "print(norm_bible[12])\n",
    "print(norm_bible[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12438\n",
      "29251\n",
      "[('shall', 1), ('unto', 2), ('lord', 3), ('thou', 4), ('thy', 5), ('god', 6), ('ye', 7), ('said', 8), ('thee', 9), ('upon', 10)]\n",
      "[(1, 'shall'), (2, 'unto'), (3, 'lord'), (4, 'thou'), (5, 'thy'), (6, 'god'), (7, 'ye'), (8, 'said'), (9, 'thee'), (10, 'upon')]\n"
     ]
    }
   ],
   "source": [
    "# keras requires the tensorflow module. Must pip install it. \n",
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "# \"The CBOW model architecture tries to predict the current target word (the center word) \n",
    "#  based on the source context words (surrounding words).\"\n",
    "# To implement the Continuous Bag of Words (CBOW) Model, we need to:\n",
    "\"\"\"\n",
    "- Build the corpus vocabulary\n",
    "- Build a CBOW (context, target) generator\n",
    "- Build the CBOW model architecture\n",
    "- Train the Model\n",
    "- Get Word Embeddings\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_bible)\n",
    "word2id = tokenizer.word_index\n",
    "word2id['PAD'] = 0\n",
    "# This is a reversed index. The regular word_index associates words with unique codes. \n",
    "id2word = {v:k for k,v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2\n",
    "print(vocab_size)\n",
    "print(len(wids))\n",
    "print(list(word2id.items())[:10])\n",
    "print(list(id2word.items())[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
