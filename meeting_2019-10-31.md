# Summary until 2019-10-31

## Activities

- We read a number of articles (the list is in the Google Drive document and Github repository). The papers specifically focused on programming by voice either propose use a prexisting one or new language. Most work fits into the first category. They adopt one of the following approaches: (i) character by character dictation, with strong macro support like "create a method" (this is the current state of the practice); (ii) direct dictation of programming language terms, with some adaptations (the work of [Begel and Graham](https://github.com/fernandocastor/voiceProgramming/blob/master/Begel_2005_SP.md) follows along these lines, whereas the old paper by [Legget and Williams](https://github.com/fernandocastor/voiceProgramming/blob/master/Legget_1984_EIV.md) seems to be more literal). As for the second category, I only know of the work by Gordon and Luger, which proposes a new programming language based on natural language. To me, looks like a more verbose Pascal. There is also work that focuses more on the tools, such as [Snell](https://github.com/fernandocastor/voiceProgramming/blob/master/Snell_2000_IPV.md)'s. She does focus on language-specific issues, such as dealing with spacing. 
  
- Played with parsing in Python. It is very straightforward.

- Played with speech recognition in Python, using both local and Internet-based recognizers. Very quickly analyzed three different sources for speech recognition, Google, Wit.ai, and CMU Sphinx. The former is the best, but it is a paid service. In any case, it is straightforward to build a simple speech recognition system in Python using the language's preexisting libraries. Some hassle installing dependencies, but no issue. Quick summary in https://github.com/fernandocastor/voiceProgramming/blob/master/Amos_2017_UGS.md.

- We had a meeting a few days ago. Defined that our current scope is limited to input. Also, we want to initially focus on a very small subset of Python with variables, functions (declarations and calls), conditions, expressions, and assignment. After that, loops and arrays. Addressing the latter seems challenging, but it is necessary to apply assessments (eg. using students in Programming I). In addition, we plan to add a simple code navigation support.


## Observations

- Previous work (e.g., [Begel and Graham 2004](https://github.com/fernandocastor/voiceProgramming/blob/master/Begel_2004_LAT.md)) has attempted to deal with ambiguity by improving the parser (using GLR parsers, instead of LR). This is an interesting solution to address the problem of homophones, e.g., "i" and "eye".

- I think that, to improve upon existing character-by-character approaches we need to accept input in English. Speech recognition is very good for English but not so much for "controlled" natural language, such as seems to be expected by existing approaches. Maybe NLP approaches can be leveraged to identify the relevant pieces of a sentence and, in combination with large synonym dictionaries such as WordNet, we can extract the relevant information. That's my main insight/bet. 

- A general obstacle is that tokenization is difficult in the presence of ambiguity if it is not in some way informed (or subsumed) by parsing. This has been adopted in the past in the context of scanner-less parsers (Eelco Visser's work maybe pioneered this)

- One thing that worries me is multi-word homophones. I've not seen a single approach that deals with this because it is hard. Maybe we don't want to tackle such as specific case.

- About the **semantic** analysis, Hidehiko suggested that there are two possible approaches: (i) to have a model that defines stuff such as what are the words that can be used to express a function declaration, a variable declaration, etc.; (ii) to train a set of neural networks to recognize the different parts of each instruction in the program.

